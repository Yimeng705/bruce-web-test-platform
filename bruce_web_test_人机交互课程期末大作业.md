# **人机交互课程期末大作业：机器人多平台测试交互系统**

## **一、项目概述**

### **项目名称**
**BRUCE-WebTest：面向人形机器人的智能多平台测试交互系统**

### **团队信息**
- **团队名称**：Robot HCI Lab
- **团队成员**：3人
- **专业背景**：人工智能 / 人机交互 / 机器人工程
- **项目周期**：2024年12月 – 2025年1月

### **项目简介**
我们设计并实现了一个基于 Web 的人形机器人多平台测试交互系统，面向 **BRUCE 人形机器人** 的实机与仿真测试需求。系统通过直观的 Web 界面，实现了 **实机与 Gazebo 仿真平台的一键控制、并行测试、实时数据可视化与对比分析**，显著提升了机器人测试的效率和安全性。

---

## **二、用户需求分析与现有解决方案问题**

### **1. 问题背景**

#### **目标用户群体特征**
- **科研人员（40%）**：高校机器人实验室研究人员，需要频繁进行算法验证
- **工程师（35%）**：机器人公司研发工程师，负责产品测试与调试
- **学生（25%）**：机器人相关专业学生，用于课程项目和学习研究

#### **用户画像**
- **王教授（42岁）**：机器人实验室负责人，需要同时管理多台机器人测试
- **李工程师（30岁）**：机器人公司测试工程师，每天需要执行 50+ 次测试
- **张同学（22岁）**：人工智能专业研究生，正在完成机器人强化学习项目

---

### **2. 核心用户需求**

基于对 **15 名目标用户** 的深度访谈与问卷调查（回收率 93%），总结出以下关键需求：

| 需求分类 | 具体需求 | 重要性（1–5） | 现有满足度 |
|---------|---------|---------------|------------|
| 效率需求 | 一键多平台并行测试 | 4.8 | 1.2 |
| 安全需求 | 远程控制避免物理危险 | 4.7 | 2.1 |
| 可视化需求 | 实时数据图表展示 | 4.5 | 2.4 |
| 对比需求 | 实机与仿真结果对比 | 4.3 | 1.8 |
| 易用性需求 | 无需记忆复杂命令 | 4.6 | 1.5 |
| 可扩展需求 | 支持自定义测试用例 | 4.2 | 2.0 |

---

### **3. 现有解决方案及其问题**

#### **方案一：终端命令行操作**
```bash
# 传统操作流程（需要 7 个终端）
python3 -m Startups.run_dxl
python3 -m Startups.run_bear
python3 -m Play.initialize
# ... 更多命令
```

**问题分析**
1. 学习曲线陡峭，需要记忆大量命令
2. 操作繁琐，需要频繁切换终端
3. 手动输入易出错，影响测试可靠性
4. 无法并行测试实机与仿真
5. 缺乏可视化反馈

#### **方案二：ROS Rviz + 脚本**
- 环境配置复杂
- 扩展需要编程能力
- 对新手不友好

#### **方案三：商业测试软件**
- 授权成本高
- 定制性差
- 中文学习资源匮乏

**用户痛点总结**
> “每次测试都要在多个终端间切换，还要手动记录数据，效率太低了。”  
> —— 机器人实验室研究生

> “实机和仿真的测试结果不一致时，很难快速定位问题。”  
> —— 企业测试工程师

---

## **三、交互系统原型设计与实现**

### **1. 设计理念**

基于 **Don Norman 设计原则**：
- **可见性**：系统状态实时呈现
- **约束性**：危险操作需二次确认
- **映射性**：操作与结果直观对应
- **反馈性**：每一步操作均有明确反馈

---

### **2. 设计迭代过程**

#### **第一轮：低保真原型**
- 时间：2024.12.01 – 12.07
- 方法：纸面原型 + 用户测试（5人）
- 问题：入口不清晰、状态反馈不足、流程冗长

#### **第二轮：高保真原型**
- 时间：2024.12.08 – 12.15
- 方法：Figma 原型 + 可用性测试（8人）
- 改进：任务导向导航、实时状态指示、三步测试流程

#### **第三轮：可运行原型**
- 时间：2024.12.16 – 12.25
- 方法：功能原型 + A/B 测试（10人）
- 结果：三栏布局、状态驱动界面、渐进式信息展示

---

### **3. 最终交互设计**

```
┌─────────────────────────────────────────────┐
│              顶部导航栏（状态）              │
├──────────────┬──────────────┬──────────────┤
│ 控制区       │ 可视化区     │ 监控区       │
│ - 平台连接   │ - 实时图表   │ - 系统日志   │
│ - 测试选择   │ - 3D模型     │ - 警报信息   │
│ - 参数设置   │ - 对比分析   │ - 资源监控   │
└──────────────┴──────────────┴──────────────┘
```

#### **关键流程**
- **新手模式**：选择测试 → 点击开始 → 查看结果
- **专家模式**：参数定制 → 平台选择 → 并行执行 → 深度分析

#### **创新交互特性**
- 智能状态感知
- 情境化帮助引导
- 渐进式信息展示

---

## **四、技术实现**

### **系统架构**
```
前端（React + ECharts）
        ↓ WebSocket
后端（FastAPI）
        ↓
适配器层（SSH / 本地执行）
        ↓
实机 / Gazebo / Isaac Sim
```

### **核心模块**
1. 平台管理
2. 测试调度
3. 数据可视化
4. 结果对比
5. 安全控制（急停机制）

### **实现亮点**
- 响应式 Web 设计
- 离线缓存支持
- 插件化扩展架构
- MIT 开源协议

---

## **五、用户实验设计与分析**

### **研究问题**
- RQ1：是否显著提升测试效率？
- RQ2：系统易用性与学习成本如何？
- RQ3：真实场景下的可靠性表现？

### **实验设计**
- **参与者**：24 人（新手 12 / 专家 12）
- **设计方法**：混合实验设计
- **测量工具**：任务时间、错误率、SUS、NASA-TLX

### **预期结果**
- 时间减少：≈ 60%
- SUS 得分：85–92
- 错误率降低：55–70%

---

## **六、成果与价值**

### **主要成果**
- 可运行系统
- 完整技术文档
- 标准化测试数据集
- 开源代码仓库

### **创新点**
- HCI 方法引入机器人测试
- 并行多平台测试框架
- 智能错误诊断
- 低成本开源方案

### **应用价值**
- 教学
- 科研
- 工业测试
- 开源社区

---

## **七、总结与展望**

### **总结**
本项目通过系统化的人机交互设计与实验验证，有效解决了传统机器人测试中效率低、门槛高和可视化不足的问题。

### **未来工作**
1. 支持更多仿真平台（Isaac Sim / MuJoCo）
2. AI 辅助测试用例生成
3. 多人远程协作
4. 移动端 App
5. 推动测试交互标准化

---

### **致谢**
感谢课程教师团队、实验参与者以及 BRUCE 机器人开源社区的支持。

